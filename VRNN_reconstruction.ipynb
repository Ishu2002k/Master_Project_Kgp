{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a523026a-bd2f-40d4-8c85-27076154620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(\"/home/jovyan/video-storage/amit_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcff1a9-4eeb-4b4b-8334-c2c55380dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:31:55.002033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-25 16:31:56.196963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "import tensorflow.compat.v1 as tf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8187dd-5a8e-4f4b-9362-fce2778a677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import ModifiedKPDetector, KPDetector\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "from Motion_Transfer_Keypoints_Prediction.Keypoints_Prediction.Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "\n",
    "\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106faa2-f8ed-4c97-b73c-f3149dc15c5a",
   "metadata": {},
   "source": [
    "# Load the pretrained VRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7f611d-9e48-4048-bc99-de3d31dc0594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:32:03.745248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10437 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:44:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "model = build_vrnn\n",
    "frames = 24\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# Load saved model:\n",
    "cfg = get_config()\n",
    "checkpoint_path = \"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" \n",
    "# checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_6-6.ckpt\" \n",
    "\n",
    "# Loads the weights\n",
    "train_model.load_weights(checkpoint_path)\n",
    "train_model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5204c-71e8-4a3e-bb94-63582d645b46",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e860938-d766-42d5-ae35-f7d081e632a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0147-2265-44b7-bc3d-cd358e3f3051",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118d6e64-05d3-40db-9d1c-11f3988bc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5c2ec-fb15-41f3-bf51-225b6a1c9d45",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d69dc76-4d13-48e5-bf20-15f2625c13a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    data = kp_list_test[video_idx]\n",
    "    data_length = len(kp_list_test[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610be487-5ada-41bc-9229-13d30cce99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c52585c-9907-4d81-b807-663fcb2313e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aeef33-b54d-4b81-86ce-cde88ccc13a5",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39667ac-30b7-4aef-ba1c-f594ba75d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 11s 27ms/step\n",
      "(529, 24, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "pred = train_model.predict(validation_data_tensor)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91b410-12df-45f1-88be-c84bf06ffb78",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb73a14-1eff-442f-a26a-174d310c0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches of each video: 44\n"
     ]
    }
   ],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bdf619-7d25-4f77-b6a4-9e10b762f9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b13a49c-d66e-438a-86bc-3548581b2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5fb41-59e4-4bdf-be1b-65a69560210a",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0027d637-1962-4151-a8c6-0a293a081636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### call the config functions and inference dataloader #########\n",
    "# config=\"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/config/abs-vox.yml\"\n",
    "\n",
    "# # Test dataset\n",
    "# with open(config) as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "# dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "# print(len(dataset))\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# # Add this before loading the checkpoint\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# ### call the functions        \n",
    "# generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "#                                         **config['model_params']['common_params'])\n",
    "# kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "#                             **config['model_params']['common_params'])\n",
    "\n",
    "# # log_dir=\"./log/test-reconstruction-vox\"\n",
    "# log_dir=\"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/log/test-reconstruction-vox\"\n",
    "# checkpoint=\"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Trained_Models/vox-cpk.pth\"\n",
    "\n",
    "# if checkpoint is not None:\n",
    "#     Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "# else:\n",
    "#     raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "# def save_obj(obj, name ):\n",
    "#     with open('./'+ name + '.pkl', 'wb') as f:\n",
    "#         pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# def load_obj(name ):\n",
    "#     with open('./' + name + '.pkl', 'rb') as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "# log_dir = os.path.join(log_dir, 'prediction')\n",
    "  \n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)\n",
    "\n",
    "# if not os.path.exists(png_dir):\n",
    "#     os.makedirs(png_dir)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     generator = DataParallelWithCallback(generator)\n",
    "#     kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "# generator.eval()\n",
    "# kp_detector.eval()\n",
    "\n",
    "# prediction_params = config['prediction_params']\n",
    "\n",
    "# num_epochs = prediction_params['num_epochs']\n",
    "# lr = prediction_params['lr']\n",
    "# bs = prediction_params['batch_size']\n",
    "# num_frames = prediction_params['num_frames']\n",
    "# loss_list_total = []\n",
    "# fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e415f557-e450-4095-979b-b7d72ff453dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', '.ipynb_checkpoints']\n",
      "Dataset size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Trained_Models/vox-cpk.pth\n",
      "Generator state loaded successfully\n",
      "Keypoint detector state loaded successfully\n",
      "GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "Total GPU memory: 10.90 GB\n",
      "Available GPU memory: 0.24 GB reserved, 0.23 GB allocated\n",
      "Setup complete. Ready for prediction/inference.\n"
     ]
    }
   ],
   "source": [
    "# Set memory allocation configuration to avoid fragmentation\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "\n",
    "config_path = \"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/config/abs-vox.yml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "    # Test dataset\n",
    "dataset = FramesDataset(is_train=False, **config['dataset_params'], mode=\"RNN\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    \n",
    "# First create models on CPU, then move to GPU if needed\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "# Initialize generator and keypoint detector\n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                             **config['model_params']['common_params'])\n",
    "    \n",
    "    # Set up logging directories\n",
    "log_dir = \"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/log/test-reconstruction-vox\"\n",
    "checkpoint_path = \"/home/jovyan/video-storage/amit_files/Motion_Transfer_Keypoints_Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Trained_Models/vox-cpk.pth\"\n",
    "    \n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "    \n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "    \n",
    "    # Clear GPU cache before loading models\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Custom checkpoint loading with CPU map location\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        # Load checkpoint to CPU first\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        # Load state dictionaries from checkpoint\n",
    "    if 'generator' in checkpoint:\n",
    "        generator.load_state_dict(checkpoint['generator'])\n",
    "        print(\"Generator state loaded successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Generator state not found in checkpoint\")\n",
    "            \n",
    "    if 'kp_detector' in checkpoint:\n",
    "        kp_detector.load_state_dict(checkpoint['kp_detector'])\n",
    "        print(\"Keypoint detector state loaded successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Keypoint detector state not found in checkpoint\")\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "    # Move models to device (GPU if available) after loading\n",
    "generator = generator.to(device)\n",
    "kp_detector = kp_detector.to(device)\n",
    "    \n",
    "    # If GPU is available, wrap models with DataParallel\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "    \n",
    "    # Get prediction parameters\n",
    "prediction_params = config['prediction_params']\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []\n",
    "    \n",
    "    # Print GPU memory information if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB reserved, {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB allocated\")\n",
    "    \n",
    "print(\"Setup complete. Ready for prediction/inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27621948-8d61-4a79-9ca7-d4b1af6476a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.11/site-packages (2.31.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (1.24.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (10.0.1)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Obtaining dependency information for imageio-ffmpeg from https://files.pythonhosted.org/packages/a0/2d/43c8522a2038e9d0e7dbdf3a61195ecc31ca576fb1527a528c877e87d973/imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (5.9.5)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Install the FFMPEG backend (recommended for most cases)\n",
    "!pip install \"imageio[ffmpeg]\"\n",
    "\n",
    "# Option 2: Install the PyAV backend (alternative)\n",
    "# !pip install \"imageio[pyav]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580aa2cf-13a2-4020-bb0d-3d6df77c7f87",
   "metadata": {},
   "source": [
    "# Original Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e2560-e807-48ed-89b7-0cee7c0638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########  FOMM+VRNN ########\n",
    "\n",
    "# for it, x in tqdm(enumerate(dataloader)):\n",
    "#         if config['reconstruction_params']['num_videos'] is not None:\n",
    "#             if it > config['reconstruction_params']['num_videos']:\n",
    "#                 break\n",
    "#         with torch.no_grad():\n",
    "#             predictions = []\n",
    "#             visualizations = []\n",
    "\n",
    "#             ######## keypoints ########\n",
    "#             kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "#             kp_driving_video = torch.tensor(kp_driving_video)\n",
    "#             kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "#         ##### Start generator\n",
    "#         loss_list = []\n",
    "#         fvd_list = []\n",
    "#         for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "#             source = x['video'][:, :, 0]\n",
    "#             driving = x['video'][:, :, i]\n",
    "#             kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "#             kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "#             kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "#             out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "#             out['kp_source'] = kp_source\n",
    "#             out['kp_driving'] = kp_driving\n",
    "#             del out['sparse_deformed']\n",
    "#             predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "#             visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "#                                                                                     driving=driving, out=out)\n",
    "#             visualizations.append(visualization)\n",
    "#             # mse loss\n",
    "#             if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "#                 loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "#                 # Calculate FVD for each frame using ground truth and predicted videos\n",
    "#                 ground_truth_features = driving.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "#                 predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "#                 fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "#         print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "#         loss_list_total.append(np.mean(loss_list))\n",
    "\n",
    "#         print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "#         fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "#         predictions = np.concatenate(predictions, axis=1)\n",
    "#         imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "#         image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "#         imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "# print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "# print(\"mean FVD score: %s\" % np.mean(fvd_list_total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fce61-d339-43b5-a4ee-7a1fead5fe52",
   "metadata": {},
   "source": [
    "<!-- # -------------------------------------------------------------------------------------------------------------- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba769e6f-bf3d-4ba4-9937-57666189a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU state:\n",
      "GPU memory allocated: 0.24 GB\n",
      "GPU memory reserved: 0.26 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video 1\n",
      "Processing frames 0 to 1\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 2 to 3\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 4 to 5\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 6 to 7\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 8 to 9\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 10 to 11\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 12 to 13\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 14 to 15\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 16 to 17\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 18 to 19\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 20 to 21\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 22 to 23\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 24 to 25\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 26 to 27\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 28 to 29\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 30 to 31\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 32 to 33\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 34 to 35\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 36 to 37\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 38 to 39\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 40 to 41\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 42 to 43\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 44 to 45\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 46 to 47\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 48 to 49\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 50 to 51\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 52 to 53\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 54 to 55\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 56 to 57\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 58 to 59\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 60 to 61\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 62 to 63\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 64 to 65\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 66 to 67\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 68 to 69\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 70 to 71\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 72 to 73\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 74 to 75\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 76 to 77\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 78 to 79\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 80 to 81\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 82 to 83\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 84 to 85\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 86 to 87\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 88 to 89\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 90 to 91\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 92 to 93\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 94 to 95\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Reconstruction loss: 0.047087926\n",
      "FVD Score: 3.3870056343985553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [3:23:00, 12180.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After video processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "\n",
      "Processing video 2\n",
      "Processing frames 0 to 1\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 2 to 3\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 4 to 5\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 6 to 7\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 8 to 9\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 10 to 11\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "After batch processing:\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n",
      "Processing frames 12 to 13\n",
      "GPU memory allocated: 0.25 GB\n",
      "GPU memory reserved: 0.26 GB\n"
     ]
    }
   ],
   "source": [
    "#########  FOMM+VRNN ########\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Clear CUDA cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Monitor memory usage\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"Initial GPU state:\")\n",
    "print_memory_usage()\n",
    "\n",
    "loss_list_total = []\n",
    "fvd_list_total = []\n",
    "\n",
    "# Process batch size - adjust based on your GPU memory\n",
    "BATCH_SIZE = 2  # Process this many frames at a time\n",
    "\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "    if config['reconstruction_params']['num_videos'] is not None:\n",
    "        if it > config['reconstruction_params']['num_videos']:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\nProcessing video {it+1}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # These lists store frame paths instead of actual frames to save memory\n",
    "        prediction_paths = []\n",
    "        visualizations = []\n",
    "        \n",
    "        ######## keypoints ########\n",
    "        # Convert to half precision to save memory\n",
    "        kp_driving_video = test_video_unstd_list[it].reshape(-1, 10, 6)\n",
    "        kp_driving_video = torch.tensor(kp_driving_video, dtype=torch.float16)\n",
    "        \n",
    "        # Source keypoints\n",
    "        kp_source = {\n",
    "            \"value\": kp_driving_video[0, :, :2].reshape(1, 10, 2).float(),  # Keep source as float32\n",
    "            \"jacobian\": kp_driving_video[0, :, 2:].reshape(1, 10, 2, 2).float()\n",
    "        }\n",
    "        \n",
    "        # Move source keypoint to GPU once\n",
    "        if torch.cuda.is_available():\n",
    "            kp_source[\"value\"] = kp_source[\"value\"].cuda()\n",
    "            kp_source[\"jacobian\"] = kp_source[\"jacobian\"].cuda()\n",
    "    \n",
    "    # Load source frame once and keep on GPU\n",
    "    source = x['video'][:, :, 0]\n",
    "    if torch.cuda.is_available():\n",
    "        source = source.cuda()\n",
    "    \n",
    "    ##### Start generator\n",
    "    loss_list = []\n",
    "    fvd_list = []\n",
    "    \n",
    "    # Calculate total frames to process\n",
    "    total_frames = ((x['video'].shape[2]) // frames) * frames\n",
    "    \n",
    "    # Create temp directory for frame storage\n",
    "    temp_dir = os.path.join(png_dir, f\"temp_{x['name'][0]}\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in small batches to avoid OOM\n",
    "    for batch_start in range(0, total_frames, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, total_frames)\n",
    "        \n",
    "        print(f\"Processing frames {batch_start} to {batch_end-1}\")\n",
    "        print_memory_usage()\n",
    "        \n",
    "        for i in range(batch_start, batch_end):\n",
    "            with torch.no_grad():  # Ensure we're not tracking gradients\n",
    "                # Load driving frame and move to GPU\n",
    "                driving = x['video'][:, :, i]\n",
    "                if torch.cuda.is_available():\n",
    "                    driving = driving.cuda()\n",
    "                \n",
    "                # Process keypoints for this frame\n",
    "                kp_driving = {\n",
    "                    \"value\": kp_driving_video[i, :, :2].reshape(1, 10, 2).float(),  # Convert to float32 for computation\n",
    "                    \"jacobian\": kp_driving_video[i, :, 2:].reshape(1, 10, 2, 2).float()\n",
    "                }\n",
    "                \n",
    "                # Move to GPU\n",
    "                if torch.cuda.is_available():\n",
    "                    kp_driving[\"value\"] = kp_driving[\"value\"].cuda()\n",
    "                    kp_driving[\"jacobian\"] = kp_driving[\"jacobian\"].cuda()\n",
    "                \n",
    "                # Generate frame\n",
    "                out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "                \n",
    "                # Add keypoints to output\n",
    "                out['kp_source'] = kp_source\n",
    "                out['kp_driving'] = kp_driving\n",
    "                \n",
    "                # Remove unnecessary large tensors\n",
    "                if 'sparse_deformed' in out:\n",
    "                    del out['sparse_deformed']\n",
    "                \n",
    "                # Save prediction directly to file instead of keeping in memory\n",
    "                frame_pred = np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "                frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n",
    "                imageio.imsave(frame_path, (255 * frame_pred).astype(np.uint8))\n",
    "                prediction_paths.append(frame_path)\n",
    "                \n",
    "                # Create and save visualization\n",
    "                visualization = Visualizer(**config['visualizer_params']).visualize(\n",
    "                    source=source.cpu(),\n",
    "                    driving=driving.cpu(), \n",
    "                    out={k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in out.items()}\n",
    "                )\n",
    "                visualizations.append(visualization)\n",
    "                \n",
    "                # Calculate metrics on CPU to save GPU memory\n",
    "                if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "                    # MSE loss\n",
    "                    loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "                    \n",
    "                    # FVD calculation\n",
    "                    ground_truth_features = driving.detach().cpu().permute(0, 2, 3, 1).reshape(256, 256, 3)\n",
    "                    predicted_features = out['prediction'].detach().cpu().permute(0, 2, 3, 1).reshape(256, 256, 3)\n",
    "                    fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "                \n",
    "                # Explicit cleanup after each frame\n",
    "                del driving, kp_driving, out, frame_pred\n",
    "                \n",
    "            # Clear cache after each frame\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Clear cache after batch\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"After batch processing:\")\n",
    "        print_memory_usage()\n",
    "    \n",
    "    # Print metrics for this video\n",
    "    print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "    loss_list_total.append(np.mean(loss_list))\n",
    "    \n",
    "    print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "    fvd_list_total.append(np.mean(fvd_list))\n",
    "    \n",
    "    # Combine predictions into a single image using PIL to save memory\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    # Combine all frames into one horizontal image\n",
    "    images = [Image.open(path) for path in prediction_paths]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    \n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    \n",
    "    # Create new image with combined dimensions\n",
    "    new_image = Image.new('RGB', (total_width, max_height))\n",
    "    \n",
    "    # Paste images side by side\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        new_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.size[0]\n",
    "    \n",
    "    # Save combined image\n",
    "    combined_path = os.path.join(png_dir, x['name'][0] + '.png')\n",
    "    new_image.save(combined_path)\n",
    "    \n",
    "    # Save animation\n",
    "    image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "    imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    import shutil\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    # Clear all memory after each video\n",
    "    del source, kp_source, kp_driving_video\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"After video processing:\")\n",
    "    print_memory_usage()\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "print(\"mean FVD score: %s\" % np.mean(fvd_list_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef605b-1872-4150-a81d-f58454bd5c21",
   "metadata": {},
   "source": [
    "# Code by Claude to run it on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22716ab-7fc3-4c40-9fd3-619ac773490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########  FOMM+VRNN ########\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import imageio\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Clear CUDA cache before starting\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Set memory allocation strategy\n",
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# # Use mixed precision to reduce memory usage\n",
    "# from torch.cuda.amp import autocast\n",
    "\n",
    "# loss_list_total = []\n",
    "# fvd_list_total = []\n",
    "\n",
    "# for it, x in tqdm(enumerate(dataloader)):\n",
    "#     if config['reconstruction_params']['num_videos'] is not None:\n",
    "#         if it > config['reconstruction_params']['num_videos']:\n",
    "#             break\n",
    "            \n",
    "#     # Process in smaller batches or frames to reduce memory usage\n",
    "#     predictions = []\n",
    "#     visualizations = []\n",
    "    \n",
    "#     ######## keypoints ########\n",
    "#     kp_driving_video = test_video_unstd_list[it].reshape(-1, 10, 6)\n",
    "#     kp_driving_video = torch.tensor(kp_driving_video)\n",
    "#     kp_source = {\n",
    "#         \"value\": kp_driving_video[0, :, :2].reshape(1, 10, 2),\n",
    "#         \"jacobian\": kp_driving_video[0, :, 2:].reshape(1, 10, 2, 2)\n",
    "#     }\n",
    "    \n",
    "#     ##### Start generator\n",
    "#     loss_list = []\n",
    "#     fvd_list = []\n",
    "    \n",
    "#     # Calculate total frames to process\n",
    "#     total_frames = (x['video'].shape[2] // frames) * frames\n",
    "    \n",
    "#     # Process frames in smaller batches to avoid OOM\n",
    "#     batch_size = 4  # Process 4 frames at a time, adjust as needed\n",
    "    \n",
    "#     for batch_start in range(0, total_frames, batch_size):\n",
    "#         batch_end = min(batch_start + batch_size, total_frames)\n",
    "        \n",
    "#         # Clear cache between batches\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#         for i in range(batch_start, batch_end):\n",
    "#             with torch.no_grad():\n",
    "#                 source = x['video'][:, :, 0]\n",
    "#                 driving = x['video'][:, :, i]\n",
    "                \n",
    "#                 kp_driving = {\n",
    "#                     \"value\": kp_driving_video[i, :, :2].reshape(1, 10, 2),\n",
    "#                     \"jacobian\": kp_driving_video[i, :, 2:].reshape(1, 10, 2, 2)\n",
    "#                 }\n",
    "                \n",
    "#                 # Use mixed precision only for the generator forward pass\n",
    "#                 with autocast():\n",
    "#                     out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "                \n",
    "#                 # Convert any half precision outputs back to float32 for visualization\n",
    "#                 out_float32 = {}\n",
    "#                 for k, v in out.items():\n",
    "#                     if isinstance(v, torch.Tensor):\n",
    "#                         out_float32[k] = v.float()  # Convert to float32\n",
    "#                     else:\n",
    "#                         out_float32[k] = v\n",
    "                \n",
    "#                 out_float32['kp_source'] = kp_source\n",
    "#                 out_float32['kp_driving'] = kp_driving\n",
    "                \n",
    "#                 # Remove unnecessary data to free memory\n",
    "#                 if 'sparse_deformed' in out_float32:\n",
    "#                     del out_float32['sparse_deformed']\n",
    "                \n",
    "#                 # Store predictions (already using CPU so no precision issues)\n",
    "#                 pred_numpy = np.transpose(out_float32['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "#                 predictions.append(pred_numpy)\n",
    "                \n",
    "#                 # Create visualization with float32 tensors\n",
    "#                 visualization = Visualizer(**config['visualizer_params']).visualize(\n",
    "#                     source=source,\n",
    "#                     driving=driving, \n",
    "#                     out=out_float32\n",
    "#                 )\n",
    "#                 visualizations.append(visualization)\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 if np.abs(out_float32['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "#                     # Calculate loss\n",
    "#                     current_loss = np.abs(out_float32['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean()\n",
    "#                     loss_list.append(current_loss)\n",
    "                    \n",
    "#                     # Calculate FVD\n",
    "#                     ground_truth_features = driving.detach().cpu().permute(0, 2, 3, 1).reshape(256, 256, 3)\n",
    "#                     predicted_features = out_float32['prediction'].detach().cpu().permute(0, 2, 3, 1).reshape(256, 256, 3)\n",
    "#                     current_fvd = compute_fvd(ground_truth_features, predicted_features)\n",
    "#                     fvd_list.append(current_fvd)\n",
    "                \n",
    "#                 # Explicitly delete tensors to free up memory\n",
    "#                 del out, out_float32, source, driving\n",
    "#                 torch.cuda.empty_cache()\n",
    "    \n",
    "#     # Print metrics\n",
    "#     print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "#     loss_list_total.append(np.mean(loss_list))\n",
    "#     print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "#     fvd_list_total.append(np.mean(fvd_list))\n",
    "    \n",
    "#     # Save outputs\n",
    "#     predictions = np.concatenate(predictions, axis=1)\n",
    "#     imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "    \n",
    "#     image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "#     imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "# print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "# print(\"mean FVD score: %s\" % np.mean(fvd_list_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b78f2-991e-4caf-8a33-a19fa0568a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c9a238-bc83-4203-afb3-07b18c95e205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

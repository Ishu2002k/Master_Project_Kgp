{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fddcd0ee-edaf-4157-b241-b491e4cc8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(\"/home/jovyan/video-storage/amit_files/\")\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(\"/home/jovyan/video-storage/amit_files/image_background_remove_tool\")\n",
    "\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28504df1-f02a-4cbe-9b28-465052b04675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.11/site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.11/site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e97edba-f064-42e7-872b-99decaf326d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carvekit.ml.arch.tracerb7.efficientnet import EfficientEncoderB7\n",
    "from carvekit.ml.wrap.tracer_b7 import TracerUniversalB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eda6924-2046-4ec1-a2d8-c7da5f6cd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TracerUniversalB7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ab7d3e0-4332-4047-a78c-31200282b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0123252-5eb5-4069-9f9a-911356cf4d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 20:04:36.930022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 20:04:37.867643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from keypoints.keypoints.data_augments import TpsAndRotate, nop\n",
    "from keypoints.keypoints.models import keynet\n",
    "from keypoints.keypoints.utils import ResultsLogger\n",
    "from keypoints.keypoints.ds import datasets as ds\n",
    "from keypoints.keypoints.config import config\n",
    "\n",
    "from keypoints.keypoints.ds.datasets import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575d8a71-6cda-4c38-bf94-2ef81225fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "args_path = '/home/jovyan/video-storage/amit_files/keypoints/configs/keypoints_celeba.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c491e3-5a29-4675-bd56-f8fb688f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef69031-c75e-4c11-9d42-e4cbf5aeaa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(device=device(type='cuda', index=0), run_id=3, comment=None, demo=False, load=None, transfer_load=None, checkpoint_freq=1000, data_root='/home/jovyan/video-storage/amit_files/Master_Thesis_Project/', config='/home/jovyan/video-storage/amit_files/keypoints/configs/keypoints_celeba.yaml', epochs=20, processes=7, seed=None, cma_algo=None, cma_step_mode='auto', cma_step_decay=0.001, cma_initial_step_size=None, cma_samples=None, cma_oversample=0, display=False, display_freq=5000, display_kp_rows=4, opt_level='O2', model_type='F', model_in_channels=3, model_keypoints=10, transporter_combine_mode='max', policy_action_select_mode='argmax', policy_depth=1, gym_reward_count_limit=None, optimizer='Adam', batch_size=16, lr=None, dataset='celeba', dataset_train_len=40000, dataset_test_len=1100, dataset_randomize=False, data_aug_tps_cntl_pts=4, data_aug_tps_variance=0.05, data_aug_max_rotate=0.1, model_z_channels=64, data_aug_type='tps_and_rotate', **{'cma_algo;': 'fast'})\n"
     ]
    }
   ],
   "source": [
    "args = config(['--config', args_path,'--data_root','/home/jovyan/video-storage/amit_files/Master_Thesis_Project/'])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ebcc99-a9b1-4828-a943-b8171ccff492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/models/keypoints/F/run_2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dir = f'data/models/keypoints/{args.model_type}/run_{args.run_id}'\n",
    "run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d7e1fc-6956-46b4-9088-1d842c165c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0 run_id: 2 comment: None demo: False load: None transfer_load: None checkpoint_freq: 1000 data_root: /home/jovyan/video-storage/amit_files/Master_Thesis_Project/celeba-low/ config: /home/jovyan/video-storage/amit_files/keypoints/configs/keypoints_celeba.yaml epochs: 20 processes: 7 seed: None cma_algo: None cma_step_mode: auto cma_step_decay: 0.001 cma_initial_step_size: None cma_samples: None cma_oversample: 0 display: False display_freq: 5000 display_kp_rows: 4 opt_level: O2 model_type: F model_in_channels: 3 model_keypoints: 10 transporter_combine_mode: max policy_action_select_mode: argmax policy_depth: 1 gym_reward_count_limit: None optimizer: Adam batch_size: 16 lr: None dataset: celeba dataset_train_len: 40000 dataset_test_len: 1100 dataset_randomize: False data_aug_tps_cntl_pts: 4 data_aug_tps_variance: 0.05 data_aug_max_rotate: 0.1 model_z_channels: 64 data_aug_type: tps_and_rotate cma_algo;: fast \n"
     ]
    }
   ],
   "source": [
    "display = ResultsLogger(run_dir=run_dir,\n",
    "                            num_keypoints=args.model_keypoints,\n",
    "                            title = 'Results',\n",
    "                            visuals = args.display,\n",
    "                            image_capture_freq = args.display_freq,\n",
    "                            kp_rows = args.display_kp_rows,\n",
    "                            comment = args.comment)\n",
    "display.header(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e86b0b9-8af2-4075-b010-5ff639c3d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" dataset \"\"\"\n",
    "datapack = ds.datasets[args.dataset]\n",
    "train, test = datapack.make(args.dataset_train_len, args.dataset_test_len, data_root=args.data_root)\n",
    "pin_memory = False if args.device == 'cpu' else True\n",
    "train_l = DataLoader(train, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=pin_memory)\n",
    "test_l = DataLoader(test, batch_size=args.batch_size, shuffle=True, drop_last=True, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9f3d19-bd8c-4acf-ab20-ab64b9dda2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model \"\"\"\n",
    "kp_network = keynet.make(args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30779542-a7a2-4a4d-b07a-dc7dd100152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeyNet(\n",
       "  (encoder): Unit(\n",
       "    (in_block): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (core): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (9): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (18): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (21): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (23): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (27): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (30): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (out_block): Sequential(\n",
       "      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (keypoint): Unit(\n",
       "    (in_block): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (core): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (9): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (18): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (20): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (21): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (23): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (27): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (30): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (out_block): Sequential(\n",
       "      (0): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ssm): SpatialLogSoftmax()\n",
       "  (key2map): GaussianLike()\n",
       "  (decoder): Unit(\n",
       "    (in_block): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(74, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (core): Sequential(\n",
       "      (0): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
       "      (5): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (6): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
       "      (14): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (21): ReLU(inplace=True)\n",
       "      (22): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')\n",
       "      (23): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (24): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): ReplicationPad2d((1, 1, 1, 1))\n",
       "      (28): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (30): ReLU(inplace=True)\n",
       "    )\n",
       "    (out_block): Sequential(\n",
       "      (0): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eebd7e1-c985-4cc0-872e-3ab1d01001f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(kp_network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d619a30-6e8f-4677-ae8e-2d420c7dcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" data augmentation \"\"\"\n",
    "if args.data_aug_type == 'tps_and_rotate':\n",
    "    augment = TpsAndRotate(args.data_aug_tps_cntl_pts, args.data_aug_tps_variance, args.data_aug_max_rotate)\n",
    "else:\n",
    "    augment = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cac5b5e-cc97-4aa6-af00-14f0f54f0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# loss function\n",
    "def l2_reconstruction_loss(x, x_, threshold = 0.50, foreground_mask=None):\n",
    "    # Calculate L2 loss\n",
    "    loss = (x - x_) ** 2\n",
    "    \n",
    "    # Apply the foreground mask if provided\n",
    "    if foreground_mask is not None:\n",
    "        if not isinstance(foreground_mask, torch.Tensor):\n",
    "            foreground_mask = torch.tensor(foreground_mask)\n",
    "        # Use torch.where for PyTorch tensors\n",
    "        loss = torch.where(foreground_mask >= threshold, loss * foreground_mask, loss * 0.5)\n",
    "    \n",
    "    # Return the mean loss\n",
    "    return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d94de291-0e67-4b31-8ca8-4224feaade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n",
    "threshold = 0.5\n",
    "\n",
    "# loss function\n",
    "# def l2_reconstruction_loss(x, x_, threshold = 0.50,foreground_mask=None):\n",
    "#     loss = (x - x_) ** 2\n",
    "#     if foreground_mask is not None:\n",
    "#         loss = np.where(foreground_mask >= threshold, loss * foreground_mask, loss * 0.5)\n",
    "#         # loss = loss * loss_mask\n",
    "#     return torch.mean(loss)\n",
    "\n",
    "criterion = l2_reconstruction_loss\n",
    "\n",
    "def to_device(data, device):\n",
    "    return tuple([x.to(device) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cabc5d8f-9a5d-42e2-b75f-a2fadc8276c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Initialize the loss file before training\n",
    "loss_file_path = run_dir + '/losses.json'\n",
    "with open(loss_file_path, 'w') as f:\n",
    "    json.dump({'train_losses': [], 'val_losses': []}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7867ff5d-244a-40ab-9d0e-84cbb1b306ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 0/20 [00:00<?, ?it/s]\n",
      "  0%|                                                                                | 0/2500 [00:03<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                  | 0/20 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=128x128 at 0x7FB1F03BCA90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of Image",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m     32\u001b[0m     x_t, z, k, m, p, heatmap \u001b[38;5;241m=\u001b[39m kp_network(x, x_)\n\u001b[0;32m---> 33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforeground_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforeground_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m     num_train_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36ml2_reconstruction_loss\u001b[0;34m(x, x_, threshold, foreground_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreground_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(foreground_mask, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 12\u001b[0m         foreground_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforeground_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Use torch.where for PyTorch tensors\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(foreground_mask \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold, loss \u001b[38;5;241m*\u001b[39m foreground_mask, loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of Image"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Main training loop\n",
    "for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    num_train_batches = 0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    if not args.demo:\n",
    "        # Training\n",
    "        # batch = tqdm(trip_train, total=len(train_t) // args.batch_size)\n",
    "        batch = tqdm(train_l, total=len(train) // args.batch_size)\n",
    "        for i, data in enumerate(batch):\n",
    "            data = to_device(data, device=args.device)\n",
    "            # x, x_ = data[0].to(args.device), data[1].to(args.device)\n",
    "            x, x_, loss_mask = augment(*data)\n",
    "            # print(x_.shape)   \n",
    "            # x__ = x_.detach().cpu().numpy()\n",
    "            x__ = np.transpose(x_.detach().cpu().numpy(),(0, 2, 3, 1)).astype(np.uint8)\n",
    "            # x__ = x__.astype(np.uint8)\n",
    "            # print(x__.shape)\n",
    "            x__ = [Image.fromarray(x__[i]) for i in range(x__.shape[0])]\n",
    "            # print(type(x__))\n",
    "            foreground_mask = model(x__)\n",
    "            print(foreground_mask[0])\n",
    "            \n",
    "            optim.zero_grad()\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                x_t, z, k, m, p, heatmap = kp_network(x, x_)\n",
    "                loss = criterion(x_t, x_, threshold = threshold,foreground_mask = foreground_mask)\n",
    "                epoch_train_loss += loss.item()\n",
    "                num_train_batches += 1\n",
    "\n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optim)    # Update model parameters\n",
    "            scaler.update()       # Adjust the scale for next iteration\n",
    "\n",
    "            if i % args.checkpoint_freq == 0:\n",
    "                kp_network.save(run_dir + '/checkpoint')\n",
    "                \n",
    "            display.log(batch, epoch, i, loss, optim, x, x_, x_t, heatmap, k, m, p, loss_mask=None, type='train', depth=20)\n",
    "\n",
    "    # Calculate average train loss for the epoch\n",
    "    epoch_train_loss /= max(1, num_train_batches)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        batch = tqdm(test_l, total=len(test) // args.batch_size)\n",
    "        for i, data in enumerate(batch):\n",
    "            data = augment(*data)\n",
    "            x, x_, loss_mask = augment(*data)\n",
    "            foreground_mask = model(x_)\n",
    "\n",
    "            with autocast():\n",
    "                x_t, z, k, m, p, heatmap = kp_network(x, x_)\n",
    "                loss = criterion(x_t, x_, threshold = threshold,foreground_mask = foreground_mask)\n",
    "                epoch_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "            display.log(batch, epoch, i, loss, optim, x, x_, x_t, heatmap, k, m, p, loss_mask=None, type='test', depth=20)\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    epoch_val_loss /= max(1, num_val_batches)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # Save the updated train and val losses after each epoch\n",
    "    with open(loss_file_path, 'r+') as f:\n",
    "        loss_data = json.load(f)\n",
    "        loss_data['train_losses'].append(epoch_train_loss)\n",
    "        loss_data['val_losses'].append(epoch_val_loss)\n",
    "        f.seek(0)  # Go to the beginning of the file\n",
    "        json.dump(loss_data, f, indent=4)  # Write updated data\n",
    "        f.truncate()  # Remove any leftover data from previous writes\n",
    "\n",
    "    ave_loss, best_loss = display.end_epoch(epoch, optim)\n",
    "\n",
    "    # Save if model improved\n",
    "    if ave_loss <= best_loss and not args.demo:\n",
    "        kp_network.save(run_dir + '/best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb30a6a-f060-4337-8fbf-10c6a3d22ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4456f0c-5206-4b0a-996e-4350cb106310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
